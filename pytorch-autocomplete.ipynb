{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-autocomplete.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Euv59djukq0w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Autocomplete with PyTorch and RNNs on Colaboratory\n",
        "\n",
        "Despite the recent very impressive advances in deep learning such as [GPT-2](https://openai.com/blog/better-language-models/), [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) or [Nvidia's GAN-powered face generator](https://medium.com/syncedreview/gan-2-0-nvidias-hyperrealistic-face-generator-e3439d33ebaf), the most influential article about deep learning for me personally is Andrej Karpathy‘s blog entry on [the unreasonable effectiveness of recurrent neural networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). When I first read the article almost four years ago, I was amazed of how easily these networks produce text passages which resemble the basic style patterns of e.g. a work by Shakespeare or Linux source code. However, when I tried to train my own recurrent neural networks (RNN), I quickly noticed that without a state-of-art GPU, my neural network sounded more like a toddler than like Shakespeare. \n",
        "\n",
        "While buying your own GPU might be too expensive to just play around with a few deep learning examples and setting up an [AWS](https://aws.amazon.com/) instance might be too much of a hassle, there is an easy-to-use alternative: [Google Colaboratory](https://colab.research.google.com), a hosted Python development environment that features free GPU (and [TPU](https://cloud.google.com/tpu/)) usage. This tutorial can be opened in Colaboratory by simply clicking on the following badge:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/christophmark/pytorch-autocomplete/blob/master/pytorch-autocomplete.ipynb)\n",
        "\n",
        "This tutorial aims to cover the complete workflow of a deep learning application, using only Google's computing and data storage resources. The content of this tutorial is inspried by Andrej Karpathy's examples: we will devise a recurrent neural network based on the deep learning framework `PyTorch` that is trained on topic-specific Python source code (e.g. code examples of `tensorflow` or `PyTorch` models) and subsequently serves as a helpful auto-complete function for coding.\n",
        "\n",
        "To implement this application, we will learn to...\n",
        "- connect the Colaboratoy notebook to Google Drive for data storage\n",
        "- scrape training data from Github using automated search queries\n",
        "- define a stacked LSTM recurrent neural network using PyTorch\n",
        "- write a GPU-accelerated training routine with a live progress visualization\n",
        "- integrate the trained model with the notebook environment to employ the auto-complete feature\n",
        "\n",
        "**Acknowledgements**\n",
        "\n",
        "Parts of the code in this tutorial have been taken from or are inspired by existing [GitHub](https://github.com/) projects & [StackOverflow](https://stackoverflow.com) answers:\n",
        "- [neural_complete](https://github.com/kootenpv/neural_complete) by [Pascal van Kooten](https://github.com/kootenpv): A very similar project, based on `keras`/`tensorflow`. The webscraping code in this tutorial is adapted from *neural_complete*.\n",
        "- [pytorch-charRNN](https://github.com/mcleonard/pytorch-charRNN) by [Mat Leonard](https://github.com/mcleonard): A PyTorch implementation of a character-wise RNN that served as the basis for the stacked LSTM network in this tutorial.\n",
        "- The integration of the custom auto-complete function would not have been possible without [this hack](https://stackoverflow.com/questions/48187554/extended-information-for-an-ipython-custom-completer) by [Tarun Lalwani](https://stackoverflow.com/users/2830850/tarun-lalwani).\n",
        "\n",
        "**Disclaimer**\n",
        "\n",
        "Feel free to use this work for your own projects, it is licensed under the [MIT License](https://opensource.org/licenses/MIT):\n",
        "\n",
        "<small><em>\n",
        "> Copyright 2019 Christoph Mark\n",
        "\n",
        "> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "> The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</em></small>"
      ]
    },
    {
      "metadata": {
        "id": "BPm1M5p94HMP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Google Colaboratory\n",
        "\n",
        "Google Colaboratory is a hosted Python development environment based on [Jupyter notebooks](https://jupyter.org/). These notebooks allow to interweave structured text (using the [markdown syntax](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)) and Python code. They and are particularly suited for prototyping new ideas, due to interactive code execution."
      ]
    },
    {
      "metadata": {
        "id": "Bza9qCDo4KjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Activate GPU support\n",
        "\n",
        "To activate GPU support, click on `Runtime > Change runtime type` int he notebook menu and choose `GPU` as the hardware accelerator. To check whether the GPU is available for computation, we import the deep learning framework [PyTorch](https://pytorch.org/):"
      ]
    },
    {
      "metadata": {
        "id": "on51uOHYOiBU",
        "colab_type": "code",
        "outputId": "e96d2831-ac4e-4291-d509-c699470a0384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "5ioJM7qIO7fJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If successful, the output of the cell above should print `True`. Note that Google Colaboratory also offers [TPU](https://cloud.google.com/tpu/) support. These *Tensor Processing Units* are specifically designed for machine learning tasks and may outperform conventional GPUs. While support for TPUs in PyTorch is still pending, [tensorflow](https://www.tensorflow.org/) models may benefit from using TPUs (see [this tutorial](https://colab.research.google.com/notebooks/tpu.ipynb)).\n",
        "\n",
        "### 1.2 Useful commands\n",
        "\n",
        "Within the notebook environment, you can not only execute Python code, but also bash commands by prepending a `!`. For example, you can install new Python packages via the package manager `pip`. Here, we just check the installed version of PyTorch:"
      ]
    },
    {
      "metadata": {
        "id": "0qcEWn4eaeM3",
        "colab_type": "code",
        "outputId": "174d4469-1688-4b78-ab42-d28508e2b9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 1.0.1.post2\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: UNKNOWN\n",
            "Author: UNKNOWN\n",
            "Author-email: UNKNOWN\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: \n",
            "Required-by: torchvision, torchtext, fastai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ITRbjD23aicp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another useful command is `!kill -9 -1`. It will reset all running kernels and free up memory (including GPU memory). Furthermore, there are a few commands to have a closer look on the hardware spcifications, i.e. to get information about the installed CPU and GPU:"
      ]
    },
    {
      "metadata": {
        "id": "9_Wds_kBbGBK",
        "colab_type": "code",
        "outputId": "7673a203-18da-472c-d57c-82c004a8d1bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!lscpu |grep 'Model name'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmPrLsJya8Zu",
        "colab_type": "code",
        "outputId": "b5540145-61dc-4801-e838-4d5f0550b45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-8b8a8acb-3932-fbb0-7bc9-b2ba7d05ac97)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UeU4j3OPbqcw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In addition, you can check the available RAM and HDD memory:"
      ]
    },
    {
      "metadata": {
        "id": "aC4y0HqHbwdg",
        "colab_type": "code",
        "outputId": "d29d043b-a00f-4799-d69b-2ac14dfda623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo | grep 'MemAvailable'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemAvailable:   12607400 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q1SUlC_Pbzug",
        "colab_type": "code",
        "outputId": "f1d4dcb1-fc4e-49c2-e41c-7b7b30841772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!df -h / | awk '{print $4}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avail\n",
            "318G\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8_3xqMSxbL5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, one can execute the following command to get a live update on the GPU usage. This is useful to check how much of the GPU memory is in use to optimize the batchsize for training. Note that whenever the training routine in a notebook is still running, you need to execute this command in another Colaboratory notebook to get an instant response:"
      ]
    },
    {
      "metadata": {
        "id": "CmEqTw0zblFS",
        "colab_type": "code",
        "outputId": "5123c40a-90fe-4c15-cabd-4351b2607602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 17 15:36:30 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P8    25W / 149W |     11MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hoThh0oD8VNj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Mount Google Drive\n",
        "\n",
        "Another important prerequisite for training our neural network is a place to save checkpoints of the trained model and to store obtained training data. Colaboratory provides convenient access to Google Drive via the `google.colab` Python module. The following command will mount your Google Drive contents to the folder path `/content/gdrive` on the Colaboratory instance. For authentication, you have to click the generated link and paste the authorization code into the input field:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "r79E1R6MVVxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GvsJbPDqmP5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can use conventional Python packages such as `os` or `sys` to create/delete/change files in your Google Drive folders just as if you were working on your local machine.\n",
        "\n",
        "## 2. Getting training data\n",
        "\n",
        "The application we want to develop in this tutorial is a classical example of character-based RNNs: The training data consists of text passages and the label (i.e. the target to train the network on) is a single character that follows the corresponding input text passage. The idea is that with enough input passages, the network will learn the structure of the text, i.e. it will learn words, sentence structure, punctuation - all from just looking at sequences of characters. We want to use Python code snippets as input sequences that are topic-specific, e.g. Python code related to PyTorch models or Matplotlib visualization code. Once we have trained our model on such a text corpus, we will use it to generate suggestions for incomplete code paragraphs - a function that is usually referred to as *auto-complete*.\n",
        "\n",
        "The problem here is to get a large enough corpus of high-quality, topic-specific Python code. GitHub qualifies as such a source: first, the code hosted there is in most cases meant to be distributed to many people and should thus be of high quality (I'm a dreamer...). Second, GitHub is probably among the largest resources of source code available right now. In the following, we will use the [Python bindings](https://selenium-python.readthedocs.io/) of the Web Browser Automation tool [Selenium](https://www.seleniumhq.org/) to scrape the GitHub search results for suitable source code.\n",
        "\n",
        "### 2.1 Webscraping with selenium\n",
        "\n",
        "[Webscraping](https://en.wikipedia.org/wiki/Web_scraping) in general refers to the extraction of data from webpages. If you already know the URLs to query that contain the data in their HTML source code, the Python package `requests` can be used to retrieve the page source code. Scraping gets more complicated when multiple steps are required, e.g. navigating a login screen first, or in cases where data is displayed dynamically by Javascript. In the latter cases, we need to simulate the user of a browser that enters user credentials and clicks buttons, but in a fully automated fashion. Here, we use [Selenium](https://www.seleniumhq.org/) in combination with a [head-less](https://en.wikipedia.org/wiki/Headless_browser) Chrome browser to navigate GitHub.\n",
        "\n",
        "The following commands install `selenium`, the `unidecode` package (needed to decode downloaded page sources), and the head-less browser on the Colaboratory instance."
      ]
    },
    {
      "metadata": {
        "id": "9JG3nnk0SWEj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install unidecode\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install chromium-chromedriver  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2peQ4G5Y4Zv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we are ready to let Selenium browse the GitHub search results and find links to source files that we will use as training data for our Recurrent Neural Network in the next section. As an exemplary search query, we will use \n",
        "```\n",
        "torch \"import torch.nn\"\n",
        "```\n",
        "and further require Python files with a minimal file size of 30kB. Our training data thus consists of Python files that involve the `pytorch` package and the definition of neural networks. \n",
        "\n",
        "The code below automatically performs the GitHub-login (using your username and password in plain text, so BE CAREFUL SHARING THIS NOTEBOOK!!), search for the specified query and the navigate the result pages until no more results are found. On each results page, it harvests the links to the source files, which are then downloaded, cleaned (remove multiple newlines, comments, etc.), and added to the training corpus.\n",
        "\n",
        "In case you do not want to scrape training data yourself, a preprocessed training data file can be downloaded [here](https://github.com/christophmark/pytorch-autocomplete/blob/master/data/github-pytorch-30000.txt)."
      ]
    },
    {
      "metadata": {
        "id": "TQvLj_YZzMpR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import unidecode\n",
        "import lxml.html\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPLLem7fzMOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your GitHub username and password in plain-text\n",
        "USERNAME = \"XYZ\" # ERASE BEFORE SHARING THIS NOTEBOOK!!!\n",
        "PASSWORD = \"XYZ\" # ERASE BEFORE SHARING THIS NOTEBOOK!!!\n",
        "\n",
        "# search query for GitHub code search\n",
        "SEARCHQUERY = 'torch \"import torch.nn\"'\n",
        "\n",
        "# minimal file size (bytes) to consider in code search\n",
        "MINSIZE = 30000\n",
        "\n",
        "# output file name to store all obtained data in\n",
        "OUTNAME = 'github-pytorch-30000'\n",
        "\n",
        "# directory to store output file in\n",
        "OUTDIR = '/content/gdrive/My Drive/Colab Files/github-scraper'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tquTO52KAIhb",
        "colab_type": "code",
        "outputId": "411c40f0-fc14-4a32-f920-ca4262ea4c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# preprocessing function to clean up file contents (remove multiple newlines, comments, etc.)\n",
        "def clean_content(x):\n",
        "    x = unidecode.unidecode(x)\n",
        "    x = re.sub('#.*$', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub(\"'''[\\s\\S]*?'''\", '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('\"\"\"[\\s\\S]*?\"\"\"', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('^[\\t]+\\n', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('^[ ]+\\n', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('\\n[\\n]+', '\\n\\n', x, flags=re.MULTILINE)\n",
        "\n",
        "    x += '\\nEOF\\n'\n",
        "    return x\n",
        "\n",
        "# create output directory if it does not exist\n",
        "if not os.path.exists(OUTDIR):\n",
        "    os.makedirs(OUTDIR)\n",
        "\n",
        "# set options for Chrome driver used to scrape GitHub search\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# initialize Chrome instance\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "\n",
        "# navigate the login screen of GitHub\n",
        "driver.get(\"https://github.com/login\")\n",
        "driver.find_element_by_id(\"login_field\").send_keys(USERNAME)\n",
        "driver.find_element_by_id(\"password\").send_keys(PASSWORD)\n",
        "driver.find_element_by_id(\"password\").submit()\n",
        "\n",
        "# adjust the search query to include only Python source files\n",
        "query = SEARCHQUERY+' filename:*.py size:>'+str(MINSIZE)+' language:Python'\n",
        "\n",
        "# initialize variables used during scraping\n",
        "links = set()\n",
        "counts = []\n",
        "counter = 1\n",
        "data = ''\n",
        "\n",
        "# initialize progress bar\n",
        "pbar = tqdm(range(1, 101)) # GitHub search only supports 100 result pages\n",
        "for i in pbar:\n",
        "    try:\n",
        "        # try not to annoy GitHub with too frequent requests\n",
        "        sleep(np.random.uniform(2))\n",
        "        \n",
        "        # get search result page\n",
        "        url = \"https://github.com/search?p={}&q={}&ref=searchresults&type=Code&utf8=%E2%9C%93\"\n",
        "        driver.get(url.format(i, query))\n",
        "        \n",
        "        # get file links from search result page\n",
        "        tree = lxml.html.fromstring(driver.page_source)\n",
        "        page_links = list(set([x for x in tree.xpath('//a/@href') if \"/blob/\" in x and \"#\" not in x]))\n",
        "        pbar.set_description(\"Page #{}. Found {} links.\".format(i, len(page_links)))\n",
        "        \n",
        "        # if no results are found on three consecutive pages, we're done searching\n",
        "        counts.append(len(page_links))\n",
        "        if sum(counts[-3:]) == 0:\n",
        "            break\n",
        "\n",
        "        # visit file links and get source code\n",
        "        for link in page_links:\n",
        "            # leave out repetitive results\n",
        "            if link not in links:\n",
        "                try:\n",
        "                    # try not to annoy GitHub with too frequent requests\n",
        "                    sleep(np.random.uniform(2))\n",
        "                    \n",
        "                    # get and process source file contents\n",
        "                    url = \"https://github.com\" + link\n",
        "                    html = requests.get(url).text\n",
        "                    tree = lxml.html.fromstring(html)\n",
        "                    xpath = '//*[@class=\"blob-code blob-code-inner js-file-line\"]'\n",
        "                    content = \"\\n\".join([x.text_content() for x in tree.xpath(xpath)])\n",
        "                    data += clean_content(content)\n",
        "\n",
        "                    pbar.set_description(\"Page #{}. File #{}. Total data size: {:.3f}MB\".format(i, counter, len(data)/1e6))\n",
        "                    counter += 1\n",
        "                except:\n",
        "                    pass\n",
        "        \n",
        "        # update set of found page links to detect duplicates\n",
        "        links.update(page_links)\n",
        "    \n",
        "    # search can be stopped by interrupting kernel\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "\n",
        "# save results\n",
        "print('No more search results. Saving...')\n",
        "with open(OUTDIR+'/'+OUTNAME+'.txt', 'w') as f:\n",
        "    f.write(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Page #100. File #894. Total data size: 21.702MB: 100%|██████████| 100/100 [35:41<00:00, 22.52s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No more search results. Saving...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1CvHbD7bzGko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having scraped the training data (or downloaded the preprocessed training data), we may inspect parts of the data. As expected, we recognize some general Python syntax and commands, but also more specific imports and function definitions related to the Pytorch framework:"
      ]
    },
    {
      "metadata": {
        "id": "_5vrE6RlffVh",
        "colab_type": "code",
        "outputId": "2edc8dd5-d8bb-4388-96f1-8ba6ccae12a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "cell_type": "code",
      "source": [
        "with open(OUTDIR+'/'+OUTNAME+'.txt') as f:\n",
        "    data = ''.join(f.readlines())\n",
        "\n",
        "print(data[:300])\n",
        "print('\\n--------------------------------------------------------------------------------\\n')\n",
        "print(data[100000:100300])\n",
        "print('\\n--------------------------------------------------------------------------------\\n')\n",
        "print(data[300000:300300])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from __future__ import print_function\n",
            "\n",
            "import time\n",
            "\n",
            "import matplotlib\n",
            "import torch.nn as nn\n",
            "import torch.nn.init as init\n",
            "\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "import os\n",
            "import datetime\n",
            "import pandas as pd\n",
            "from torch.utils.data import TensorDataset\n",
            "\n",
            "import torch.nn.parallel\n",
            "from skle\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
            "        dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
            "        dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
            "        dataset['Fare'] = dataset['Fare'].astype(int)\n",
            "\n",
            "    for dataset in co\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " stride=stride, padding=padding,\n",
            "                      bias=bias)\n",
            "    return ret\n",
            "\n",
            "class Forecaster(nn.Module):\n",
            "    def __init__(self, num_seqs):\n",
            "        super(Forecaster, self).__init__()\n",
            "\n",
            "        num_filter = [8, 16, 16]\n",
            "        kernel_size_l = [7, 7, 5]\n",
            "        self.rnn1_1 = ConvGRUCell(input_size\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_8sF07wma37",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Deep recurrent neural networks with PyTorch\n",
        "\n",
        "In contrast to many other deep learning tutorials here on Google Colaboratory that use the `tensorflow` framework, we will use Facebook's deep learning framework `PyTorch`. It is imported as `torch` and pre-installed on Colab-machines by default."
      ]
    },
    {
      "metadata": {
        "id": "5bZ5ct3EIs-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display # live plotting\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZfasEP6UbP6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 Working with text input\n",
        "\n",
        "The aim of this project is to build a recurrent naurel network that can learn to write topic-specific Python code from series of individual characters. In contrast to word-based recurrent neural networks, this approach has the advantage that the vocabulary stays very small (the alphabet + other syntax characters), whereas word-based RNNs have to include common variable names and function names to generate meaningful output. The following functions do the preprocessing of character-based data, i.e. they encode the data (using [one-hot encoding](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)) and split them into batches for memory-efficient training of the RNN."
      ]
    },
    {
      "metadata": {
        "id": "M9HWzAkXJabf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_chars(filepath):\n",
        "    \"\"\"\n",
        "    Returns list of all characters present in a file\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    chars = list(set(data))\n",
        "    return chars\n",
        "\n",
        "\n",
        "def load_data(filepath, chars=None):\n",
        "    \"\"\"\n",
        "    Opens a data file, determines the set of characters present in the file and encodes the characters.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    if chars is None:\n",
        "        chars = tuple(set(data))\n",
        "    \n",
        "    # lookup tables for encoding\n",
        "    int2char = dict(enumerate(chars))\n",
        "    char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "    \n",
        "    # encoding\n",
        "    encoded = np.array([char2int[ch] for ch in data])\n",
        "\n",
        "    return chars, encoded\n",
        "\n",
        "\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \"\"\"\n",
        "    One-hot encoding for character-data\n",
        "    \"\"\"\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def get_batches(arr, n_seqs, n_steps):\n",
        "    \"\"\"\n",
        "    Batch generator that returns mini-batches of size (n_seqs x n_steps)\n",
        "    \"\"\"\n",
        "    batch_size = n_seqs * n_steps\n",
        "    n_batches = len(arr) // batch_size\n",
        "\n",
        "    # always create full batches\n",
        "    arr = arr[:n_batches * batch_size]\n",
        "    \n",
        "    # reshape\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], n_steps):\n",
        "        # features (sequence of characters)\n",
        "        x = arr[:, n:n + n_steps]\n",
        "        \n",
        "        # targets (the next character after the sequence)\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-qBRN2MXUrsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 Character-level RNN\n",
        "\n",
        "The following code defines a basic multi-layer RNN with [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) cells and [Dropout](https://en.wikipedia.org/wiki/Dropout_(neural_networks)) in `PyTorch`. Furthermore, we define functions to save checkpoints of the model during training and load these checkpoints again. These functions allow us to keep the best-trained model (according to the validation dataset) during the training procedure and to resume training after an interruption or to fine-tune a model on a second dataset."
      ]
    },
    {
      "metadata": {
        "id": "V7MrLJnlUq-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Basic implementation of a multi-layer RNN with LSTM cells and Dropout.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "        \"\"\"\n",
        "\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x, hidden\n",
        "\n",
        "    def predict(self, char, hidden=None, device=torch.device('cpu'), top_k=None):\n",
        "        \"\"\"\n",
        "        Given a character, predict the next character. Returns the predicted character and the hidden state.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.to(device)\n",
        "            try:\n",
        "                x = np.array([[self.char2int[char]]])\n",
        "            except KeyError:\n",
        "                return '', hidden\n",
        "\n",
        "            x = one_hot_encode(x, len(self.chars))\n",
        "            inputs = torch.from_numpy(x).to(device)\n",
        "\n",
        "            out, hidden = self.forward(inputs, hidden)\n",
        "\n",
        "            p = F.softmax(out, dim=2).data.to('cpu')\n",
        "\n",
        "            if top_k is None:\n",
        "                top_ch = np.arange(len(self.chars))\n",
        "            else:\n",
        "                p, top_ch = p.topk(top_k)\n",
        "                top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "            if top_k == 1:\n",
        "                char = int(top_ch)\n",
        "            else:\n",
        "                p = p.numpy().squeeze()\n",
        "                char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "            return self.int2char[char], hidden\n",
        "\n",
        "\n",
        "def save_checkpoint(net, opt, filename, train_history={}):\n",
        "    \"\"\"\n",
        "    Save trained model to file.\n",
        "    \"\"\"\n",
        "    checkpoint = {'n_hidden': net.n_hidden,\n",
        "                  'n_layers': net.n_layers,\n",
        "                  'state_dict': net.state_dict(),\n",
        "                  'optimizer': opt.state_dict(),\n",
        "                  'tokens': net.chars,\n",
        "                  'train_history': train_history}\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        torch.save(checkpoint, f)\n",
        "\n",
        "\n",
        "def load_checkpoint(filename):\n",
        "    \"\"\"\n",
        "    Load trained model from file.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        checkpoint = torch.load(f, map_location='cpu')\n",
        "\n",
        "    net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "    net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    return net, checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyHeBTIcU1ia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training\n",
        "\n",
        "Training + validation loop. We implement early-stopping if validation loss has not decreased for 10 epochs."
      ]
    },
    {
      "metadata": {
        "id": "7Ivpjd3mU2nN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.ion() # Allow live updates of plots\n",
        "\n",
        "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, device=torch.device('cpu'),\n",
        "          name='checkpoint', early_stop=True, plot=True):\n",
        "    \"\"\"\n",
        "    Training loop.\n",
        "    \"\"\"\n",
        "    net.train() # switch into training mode\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr) # initialize optimizer\n",
        "    criterion = nn.CrossEntropyLoss() # initialize loss function\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    net.to(device) # move neural net to GPU/CPU memory\n",
        "    \n",
        "    min_val_loss = 10.**10 # initialize minimal validation loss\n",
        "    train_history = {'epoch': [], 'step': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "    n_chars = len(net.chars) # get size of vocabulary\n",
        "    \n",
        "    # main loop over training epochs\n",
        "    for e in range(epochs):\n",
        "        hidden = None # reste hidden state after each epoch\n",
        "        \n",
        "        # loop over batches\n",
        "        for x, y in get_batches(data, n_seqs, n_steps):\n",
        "\n",
        "            # encode data and create torch-tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x).to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
        "            \n",
        "            # reset gradient information\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # generate network output\n",
        "            output, hidden = net.forward(inputs, hidden)\n",
        "            \n",
        "            # compute loss\n",
        "            loss = criterion(output.view(n_seqs * n_steps, n_chars), targets.view(n_seqs * n_steps))\n",
        "            \n",
        "            # compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # gradient clipping to prevent exploding gradients\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            \n",
        "            # optmize\n",
        "            opt.step()\n",
        "\n",
        "            # prevent backpropagating through the entire training history\n",
        "            # by detaching hidden state and cell state\n",
        "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "        \n",
        "        # validation step is done without tracking gradients\n",
        "        with torch.no_grad():\n",
        "            val_h = None\n",
        "            val_losses = []\n",
        "            \n",
        "            for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "                x = one_hot_encode(x, n_chars)\n",
        "                inputs, targets = torch.from_numpy(x).to(device), torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "                output, val_h = net.forward(inputs, val_h)\n",
        "                \n",
        "                val_loss = criterion(output.view(n_seqs * n_steps, n_chars), targets.view(n_seqs * n_steps))\n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # compute mean validation loss over batches\n",
        "            mean_val_loss = np.mean(val_losses)\n",
        "            \n",
        "            # track progress\n",
        "            train_history['epoch'].append(e+1)\n",
        "            train_history['loss'].append(loss.item())\n",
        "            train_history['val_loss'].append(mean_val_loss)\n",
        "        \n",
        "        if plot:\n",
        "            # create live plot of training loss and validation loss\n",
        "            plt.clf()\n",
        "            plt.plot(train_history['loss'], lw=2, c='C0')\n",
        "            plt.plot(train_history['val_loss'], lw=2, c='C1')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.title(\"{}   Epoch: {:.0f}/{:.0f}   Loss: {:.4f}   Val Loss: {:.4f}\".format(\n",
        "                datetime.now().strftime('%H:%M:%S'),\n",
        "                e+1, epochs,\n",
        "                loss.item(),\n",
        "                mean_val_loss), color='k')\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf())\n",
        "        else:\n",
        "            # print training progress\n",
        "            print(\"{}   Epoch: {:.0f}/{:.0f}   Loss: {:.4f}   Val Loss: {:.4f}\".format(\n",
        "                datetime.now().strftime('%H:%M:%S'),\n",
        "                e+1, epochs,\n",
        "                loss.item(),\n",
        "                mean_val_loss))\n",
        "        \n",
        "        # save model checkpoint if validation loss has decreased\n",
        "        if mean_val_loss < min_val_loss:\n",
        "            save_checkpoint(net, opt, name+'.net', train_history=train_history)\n",
        "            min_val_loss = mean_val_loss\n",
        "        \n",
        "        # if validation loss has not decreased for the last 10 epochs, stop training\n",
        "        if early_stop:\n",
        "            if e - np.argmin(train_history['val_loss']) > 10:\n",
        "                display.clear_output()\n",
        "                print('Validation loss does not decrease further, stopping training.')\n",
        "                break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64OlZ0TLZNq9",
        "colab_type": "code",
        "outputId": "5c60b8a1-7de0-4157-d369-81aa8cd2044e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "# use GPU if available, else use CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# load data\n",
        "chars, data = load_data('/content/gdrive/My Drive/Colab Files/github-scraper/github-pytorch-30000.txt')\n",
        "\n",
        "# create RNN\n",
        "net = CharRNN(chars, n_hidden=256, n_layers=3)\n",
        "\n",
        "# train\n",
        "plt.figure(figsize=(12, 4))\n",
        "train(net, data, epochs=500, n_seqs=768, n_steps=512, lr=0.001, device=device, val_frac=0.1,\n",
        "      name='/content/gdrive/My Drive/Colab Files/training-github-pytorch-30000', plot=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss does not decrease further, stopping training.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAEWCAYAAABlvlEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MWt9VlBLnKOd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Application: topic-specific autocomplete\n",
        "\n",
        "The goal is to use the trained network as an auto-completer when writing `PyTorch` code."
      ]
    },
    {
      "metadata": {
        "id": "b-1i8nt74qPF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1 Sampling the trained network\n",
        "\n",
        "A simple function to sample network output after priming the network with some Python code."
      ]
    },
    {
      "metadata": {
        "id": "6SVMsO2-nMfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def load_checkpoint(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        checkpoint = torch.load(f, map_location='cpu')\n",
        "\n",
        "    net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "    net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    train_history = checkpoint['train_history']\n",
        "\n",
        "    return net, train_history\n",
        "\n",
        "\n",
        "def sample_lines(net, n_lines=3, prime='import', top_k=None, device='cpu', max_len=1000):\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = []\n",
        "    h = None\n",
        "    for ch in prime:\n",
        "        char, h = net.predict(ch, h, device=device, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    l = 0\n",
        "    for ii in range(max_len):\n",
        "        char, h = net.predict(chars[-1], h, device=device, top_k=top_k)\n",
        "        chars.append(char)\n",
        "        if char == '\\n':\n",
        "            l += 1\n",
        "            if l == n_lines:\n",
        "                break\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def clean_string(x):\n",
        "    x = re.sub('#.*$', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub(\"'''[\\s\\S]*?'''\", '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('\"\"\"[\\s\\S]*?\"\"\"', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('^[\\t]+\\n', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('^[ ]+\\n', '', x, flags=re.MULTILINE)\n",
        "    x = re.sub('\\n[\\n]+', '\\n\\n', x, flags=re.MULTILINE)\n",
        "    return x\n",
        "\n",
        "net, _ = load_checkpoint('/content/gdrive/My Drive/Colab Files/training-github-pytorch-30000.net')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OEuJKW9EnVA-",
        "colab_type": "code",
        "outputId": "a78d842f-9ebd-4a95-9122-c496d4a1dea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "prime = 'import torch\\n'\n",
        "\n",
        "clean_prime = clean_string(prime)\n",
        "print(sample_lines(net, 5, prime=clean_prime, top_k=3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from torch.utils.data import Dataset, DataLoader\n",
            "\n",
            "def conv(predicate_encoder, data, data_train_loader, train_loader, train_loader):\n",
            "    plt.model()\n",
            "    for item in range(0, n_layers):\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kwgOOkMprDlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 Jupyter notebook magic\n",
        "\n",
        "To make sampling more convenient, we define the magic command `%%sample`. If a code cell starts with this command, the remaining code in the cell is used as input for the network. The output of the cell upon execution are the next 20 lines of code as predicted by the network."
      ]
    },
    {
      "metadata": {
        "id": "5ef0HssMooQm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
        "                                register_line_cell_magic)\n",
        "\n",
        "@register_cell_magic\n",
        "def sample(line, prime):\n",
        "    clean_prime = clean_string(prime)\n",
        "    print(sample_lines(net, 20, prime=clean_prime, top_k=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rf3CHpNuqTJL",
        "colab_type": "code",
        "outputId": "6957f8a2-cf18-4a49-c8df-95b0a531470b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "%%sample\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "import numpy as np\n",
            "\n",
            "def state_dict(self, input):\n",
            "\n",
            "    if self.start_size == 1:\n",
            "        if isinstance(self, nn.Module):\n",
            "            if self.dataset == 'critic':\n",
            "                return self._cuda()\n",
            "    def forward(self, input):\n",
            "        if not isinstance(params):\n",
            "            if self.training:\n",
            "                input = self.activation(self.conv(x))\n",
            "                save = module(state)\n",
            "\n",
            "        if self.device:\n",
            "            return self.__met___mask()\n",
            "        if self.dict == 'test':\n",
            "            target = self.transitions.compute(data)\n",
            "            self.assertTrue(target.shape)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DrzV8f3P6SCB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.3 Juypter notebook integration\n",
        "\n",
        "We define a custom completer within the Jupyter notebook environment. This injects code-completions from our trained network into the suggestions from the Jupyter editor when hitting `TAB`:"
      ]
    },
    {
      "metadata": {
        "id": "wlLndYCfqUpL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "import inspect\n",
        "\n",
        "def charRNN_completer(self, t):\n",
        "    # get code of current cell\n",
        "    locals_caller = inspect.currentframe().f_back.f_back.f_back.f_back.f_locals\n",
        "    code = locals_caller['code']\n",
        "    cursor_pos = locals_caller['cursor_pos']\n",
        "    \n",
        "    # remove any reference to avoid leakages\n",
        "    del locals_caller\n",
        "    \n",
        "    # create completions\n",
        "    clean_prime = clean_string(code)\n",
        "    completions = []\n",
        "    \n",
        "    completions.append(sample_lines(net, 1, prime=clean_prime, top_k=1))\n",
        "    \n",
        "    for i in range(3):\n",
        "        completions.append(sample_lines(net, 1, prime=clean_prime, top_k=None))\n",
        "        \n",
        "    return completions\n",
        "\n",
        "IPython.get_ipython().set_custom_completer(charRNN_completer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlY0tMoUtnQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}